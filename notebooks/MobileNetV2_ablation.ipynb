{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5cf43ec1",
      "metadata": {
        "id": "5cf43ec1"
      },
      "source": [
        "Comparing Mobile Net V2 performance with and without memory+ layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "911ee3b6",
      "metadata": {
        "id": "911ee3b6"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "import timm\n",
        "import math\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "605269c6",
      "metadata": {
        "id": "605269c6"
      },
      "outputs": [],
      "source": [
        "class MemoryPlusLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, memory_slots, top_k = 32):\n",
        "        # Define your memory mechanism here\n",
        "        # Using Berges et al. (2024) \"Memory Layers at scale\" as a reference for the memory layer design\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.key_dim = d_model // 2\n",
        "        self.subkey_dim = self.key_dim // 2\n",
        "        self.value_dim = d_model # <-- NOTE: May experiment with this value, as it may affect performance and memory usage.\n",
        "\n",
        "        # Total memory_slots = |C| * |C'|. Sub-key matrices have sqrt(memory_slots) rows.\n",
        "        self.num_subkeys = math.isqrt(memory_slots)\n",
        "        assert self.num_subkeys ** 2 == memory_slots, f\"memory_slots (n = {memory_slots}) must be a perfect square.\"\n",
        "\n",
        "        # Query MLP\n",
        "        self.query = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model * 4),\n",
        "            nn.SiLU(), # <-- Should ideally match whatever the base models FFN activation function is.\n",
        "            nn.Linear(d_model * 4, self.subkey_dim)\n",
        "        )\n",
        "\n",
        "        # Sub-Key Matrix One and Two\n",
        "        # NOTE: Don't use nn.linear here, due to sparse key retrieval mechanism in forward pass.\n",
        "        self.subkey_one = nn.Parameter(torch.empty(self.num_subkeys, self.subkey_dim, dtype=torch.float32))\n",
        "        self.subkey_two = nn.Parameter(torch.empty(self.num_subkeys, self.subkey_dim, dtype=torch.float32))\n",
        "        nn.init.uniform_(self.subkey_one, a = -1, b = 1)\n",
        "        nn.init.uniform_(self.subkey_two, a = -1, b = 1)\n",
        "\n",
        "        # Value Matrix\n",
        "        self.values = nn.Parameter(torch.empty(memory_slots, self.value_dim, dtype=torch.float32))\n",
        "        nn.init.normal_(self.values, std=0.02)  # apparently from lample et al 2019, CAN't FIND ITS REFERENCE\n",
        "\n",
        "        # Weight Matrix One\n",
        "        self.W1 = nn.Linear(d_model, self.value_dim, bias=False)\n",
        "\n",
        "        # Weight Matrix Two\n",
        "        self.W2 = nn.Linear(self.value_dim, d_model, bias=False)\n",
        "\n",
        "        # Silu Activation Function\n",
        "        self.silu = nn.SiLU()\n",
        "\n",
        "        # QK-Normalisation,\n",
        "        # NOTE:I think its more a general backbone design choice for memory layer, potentially place this after residual connection as we are using interleaved architecture (at end of this gated memory layer)\n",
        "        \"\"\"\n",
        "        NOTE: This is a technique used to stabilize training and improve convergence in transformer models.\n",
        "        \"\"\"\n",
        "        self.qk_norm = nn.RMSNorm(self.subkey_dim)\n",
        "\n",
        "        # Top-K Selection\n",
        "        \"\"\"\n",
        "        NOTE: Can experiment with this value, as it may affect performance and memory usage.\n",
        "        \"\"\"\n",
        "        self.top_k = top_k\n",
        "\n",
        "        # Softmax\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "\n",
        "    def lookup_memory(self, query):\n",
        "\n",
        "        # 1. Apply normalisation for cosine similarity style lookup\n",
        "        k1 = self.qk_norm(self.subkey_one)\n",
        "        k2 = self.qk_norm(self.subkey_two)\n",
        "\n",
        "        # 2. Get similarity subkey scores with query\n",
        "        sim_scores_1 = query @ k1.T\n",
        "        sim_scores_2  = query @ k2.T\n",
        "        all_scores = sim_scores_1.unsqueeze(-1) + sim_scores_2.unsqueeze(-2)\n",
        "\n",
        "        # 3. Cartesian Product Search:\n",
        "        all_scores = all_scores.view(*all_scores.shape[:-2], -1)\n",
        "\n",
        "        # 4. Select the final top-k combinations\n",
        "        top_k_scores, top_k_indices = torch.topk(all_scores, self.top_k, dim=-1)\n",
        "\n",
        "        # 5. Retrieve Values and Aggregate\n",
        "        s = self.softmax(top_k_scores)\n",
        "\n",
        "        # 6. Gather Values and Aggregate: NOTE: Using EmbeddingBag!\n",
        "        # TODO: Make CUDA kernel to quicken EmbeddingBag solution\n",
        "        flat_indices = top_k_indices.view(-1, self.top_k)\n",
        "        flat_weights = s.view(-1, self.top_k)\n",
        "        y_flat = F.embedding_bag(flat_indices, self.values, per_sample_weights=flat_weights, mode='sum')\n",
        "\n",
        "        return y_flat.view(*query.shape[:-1], self.value_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        q = self.query(x)\n",
        "        q = self.qk_norm(q)\n",
        "\n",
        "        y = self.lookup_memory(q)\n",
        "\n",
        "        m_plus = self.silu(self.W1(x))\n",
        "        m_plus = y * m_plus\n",
        "        m_plus = self.W2(m_plus)\n",
        "\n",
        "        return m_plus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "df1d046f",
      "metadata": {
        "id": "df1d046f"
      },
      "outputs": [],
      "source": [
        "def profile_model_performance(model, device, name=\"Model\"):\n",
        "    \"\"\"Profiles a single forward and backward pass to see memory/FLOP tradeoffs.\"\"\"\n",
        "    print(f\"\\n--- Profiling {name} ---\")\n",
        "    model.eval()\n",
        "    inputs = torch.randn(1, 3, 224, 224).to(device)\n",
        "\n",
        "\n",
        "     # [ProfilerActivity.CPU, ProfilerActivity.CUDA] if torch.cuda.is_available() else [ProfilerActivity.CPU],\n",
        "    with profile(\n",
        "        activities = [ProfilerActivity.CPU],\n",
        "        record_shapes=True,\n",
        "        profile_memory=True,\n",
        "        with_stack=True\n",
        "    ) as prof:\n",
        "        with record_function(\"forward_pass\"):\n",
        "            output = model(inputs)\n",
        "        with record_function(\"backward_pass\"):\n",
        "            loss = output.sum()\n",
        "            loss.backward()\n",
        "\n",
        "    # Sorted by CUDA time if available, else CPU time\n",
        "    sort_by = \"gpu_time_total\" if torch.cuda.is_available() else \"cpu_time_total\"\n",
        "    print(prof.key_averages().table(sort_by=sort_by, row_limit=10))\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for images, labels in loader:\n",
        "        print(\"hi\")\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "    return running_loss / len(loader), 100. * correct / total\n",
        "\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    return running_loss / len(loader), 100. * correct / total\n",
        "\n",
        "def run_comparison(dense_model, memory_model, train_loader, test_loader, device, epochs=5):\n",
        "    # Detect device (Note: MPS for Mac is an option, but profiler support varies)\n",
        "\n",
        "    # Fixed keys to match your storage logic\n",
        "    results = {'dense': {'val_loss': [], 'val_acc': []}, 'memory': {'val_loss': [], 'val_acc': []}}\n",
        "    speed_comp = {dense_model : 0.0, memory_model : 0.0}\n",
        "    for name, model in [('dense', dense_model), ('memory', memory_model)]:\n",
        "        print(f\"\\nStarting training for {name}...\")\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        start_time = time.time()\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "            train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "            print(f\"Epoch {epoch+1}: train loss: {train_loss}, train acc: {train_acc}\")\n",
        "            val_loss, val_acc = validate(model, test_loader, criterion, device)\n",
        "\n",
        "            results[name]['val_loss'].append(val_loss)\n",
        "            results[name]['val_acc'].append(val_acc)\n",
        "            print(f\"Epoch {epoch+1}: Val Acc {val_acc:.2f}%\")\n",
        "\n",
        "        end_time = time.time()\n",
        "        speed_comp[model] = end_time - start_time\n",
        "\n",
        "\n",
        "\n",
        "    return results, speed_comp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5336c528",
      "metadata": {
        "id": "5336c528"
      },
      "outputs": [],
      "source": [
        "# Plotting Accuracy\n",
        "def plot_results(results):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(results['dense']['acc'], label='Dense Baseline (Pre-trained ViT)')\n",
        "    plt.plot(results['memory']['acc'], label='Memory+ Adapter ViT')\n",
        "    plt.title('CIFAR-100 Validation Accuracy')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fa5e41fa",
      "metadata": {
        "id": "fa5e41fa",
        "outputId": "f8eba76b-07c1-40ea-d91a-320e087b2966",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Init Device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a45a8ae5",
      "metadata": {
        "id": "a45a8ae5",
        "outputId": "99eb3cea-1795-40e5-87b0-82b34eb0e275",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 10.4MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 188kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.20MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 23.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Init Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_set = datasets.FashionMNIST(root='./data_dir', train=True, download=True, transform=transform)\n",
        "test_set = datasets.FashionMNIST(root='./data_dir', train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9699f2d9",
      "metadata": {
        "id": "9699f2d9",
        "outputId": "396ea6e4-c518-42fb-a899-e8e8c1061cad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500,
          "referenced_widgets": [
            "bf97a6f093ae4474bf7ea39a57f3d5d6",
            "d89b86f645de4cd1b9c1b80bf17e5f58",
            "d9b5ab6418f847dbba071958003032a7",
            "787156de9af34710b14978072d2c84cf",
            "1d7cb56767334f7ba4bf50b8ff4da7ea",
            "2545645d56cb487fbf4a64113b749ffb",
            "180456cff1794dda828166b22c81a94b",
            "0071c09a17ae43489546672202efc88f",
            "fe0f70d67b0a48baa4c872ce4edee89d",
            "53c51f2d66cb4220bee5ef7633259803",
            "7ee96a01b39a4bbc8b297597763d484e"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/14.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf97a6f093ae4474bf7ea39a57f3d5d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'EfficientNet' object has no attribute 'embed_dim'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1603620958.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmemory_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mobilenetv2_100'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./models_dir\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0md_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdense_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmemory_slots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmemory_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMemoryPlusLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_slots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_slots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1964\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1965\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1966\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1967\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'EfficientNet' object has no attribute 'embed_dim'"
          ]
        }
      ],
      "source": [
        "# Init Models\n",
        "dense_model = timm.create_model('mobilenetv2_100', pretrained=False, num_classes=10, cache_dir=\"./models_dir\").to(device)\n",
        "memory_model = timm.create_model('mobilenetv2_100', pretrained=False, num_classes=10, cache_dir = \"./models_dir\").to(device)\n",
        "\n",
        "d_model = dense_model.embed_dim\n",
        "memory_slots = 256**2\n",
        "memory_model.blocks[6].mlp = MemoryPlusLayer(d_model=d_model, memory_slots=memory_slots).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb3b4d1b",
      "metadata": {
        "id": "bb3b4d1b"
      },
      "outputs": [],
      "source": [
        "\"\"\"VERY IMPORTANT NOTE:\n",
        "\n",
        "Base model is 17x faster than Memory+ (1024**2 memory slots) ViT!!!!\n",
        "NEED CUSTOM KERNEL FOR EMBEDDINGBAG SOLUTION TO SPEED THIS UP,\n",
        "AS THIS IS THE BOTTLENECK IN THE MEMORY LAYER.\n",
        "\n",
        "Hoever found memory slot size 256**2 to be near performance of baseline!\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# PROFILE MODELS BEFORE TRAINING TO SEE MEMORY/FLOP TRADEOFFS\n",
        "profile_model_performance(dense_model, device, name=\"Dense Baseline\")\n",
        "profile_model_performance(memory_model, device, name=\"Memory+ Adapter\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "623f648e",
      "metadata": {
        "id": "623f648e"
      },
      "outputs": [],
      "source": [
        "res, speeds = run_comparison(dense_model, memory_model, train_loader, test_loader, device, epochs=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "543e065a",
      "metadata": {
        "id": "543e065a"
      },
      "outputs": [],
      "source": [
        "plot_results(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1636bfc",
      "metadata": {
        "id": "b1636bfc"
      },
      "outputs": [],
      "source": [
        "for m in speeds.key():\n",
        "    print(f\"{m} Training Time: {speeds[m]:.2f} seconds\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bf97a6f093ae4474bf7ea39a57f3d5d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d89b86f645de4cd1b9c1b80bf17e5f58",
              "IPY_MODEL_d9b5ab6418f847dbba071958003032a7",
              "IPY_MODEL_787156de9af34710b14978072d2c84cf"
            ],
            "layout": "IPY_MODEL_1d7cb56767334f7ba4bf50b8ff4da7ea"
          }
        },
        "d89b86f645de4cd1b9c1b80bf17e5f58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2545645d56cb487fbf4a64113b749ffb",
            "placeholder": "​",
            "style": "IPY_MODEL_180456cff1794dda828166b22c81a94b",
            "value": "model.safetensors: 100%"
          }
        },
        "d9b5ab6418f847dbba071958003032a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0071c09a17ae43489546672202efc88f",
            "max": 14184144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe0f70d67b0a48baa4c872ce4edee89d",
            "value": 14184144
          }
        },
        "787156de9af34710b14978072d2c84cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53c51f2d66cb4220bee5ef7633259803",
            "placeholder": "​",
            "style": "IPY_MODEL_7ee96a01b39a4bbc8b297597763d484e",
            "value": " 14.2M/14.2M [00:01&lt;00:00, 10.4MB/s]"
          }
        },
        "1d7cb56767334f7ba4bf50b8ff4da7ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2545645d56cb487fbf4a64113b749ffb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "180456cff1794dda828166b22c81a94b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0071c09a17ae43489546672202efc88f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe0f70d67b0a48baa4c872ce4edee89d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "53c51f2d66cb4220bee5ef7633259803": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ee96a01b39a4bbc8b297597763d484e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
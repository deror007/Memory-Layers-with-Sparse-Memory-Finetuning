{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cf43ec1",
   "metadata": {},
   "source": [
    "Comparing Mobile Net V2 performance with and without memory+ layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911ee3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import timm\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605269c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryPlusLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, memory_slots, top_k = 32):\n",
    "        # Define your memory mechanism here\n",
    "        # Using Berges et al. (2024) \"Memory Layers at scale\" as a reference for the memory layer design\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.key_dim = d_model // 2\n",
    "        self.subkey_dim = self.key_dim // 2\n",
    "        self.value_dim = d_model # <-- NOTE: May experiment with this value, as it may affect performance and memory usage.\n",
    "        \n",
    "        # Total memory_slots = |C| * |C'|. Sub-key matrices have sqrt(memory_slots) rows.\n",
    "        self.num_subkeys = math.isqrt(memory_slots)\n",
    "        assert self.num_subkeys ** 2 == memory_slots, f\"memory_slots (n = {memory_slots}) must be a perfect square.\"\n",
    "\n",
    "        # Query MLP\n",
    "        self.query = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.SiLU(), # <-- Should ideally match whatever the base models FFN activation function is.\n",
    "            nn.Linear(d_model * 4, self.subkey_dim)\n",
    "        )\n",
    "        \n",
    "        # Sub-Key Matrix One and Two\n",
    "        # NOTE: Don't use nn.linear here, due to sparse key retrieval mechanism in forward pass.\n",
    "        self.subkey_one = nn.Parameter(torch.empty(self.num_subkeys, self.subkey_dim, dtype=torch.float32))\n",
    "        self.subkey_two = nn.Parameter(torch.empty(self.num_subkeys, self.subkey_dim, dtype=torch.float32))\n",
    "        nn.init.uniform_(self.subkey_one, a = -1, b = 1)\n",
    "        nn.init.uniform_(self.subkey_two, a = -1, b = 1)\n",
    "\n",
    "        # Value Matrix\n",
    "        self.values = nn.Parameter(torch.empty(memory_slots, self.value_dim, dtype=torch.float32))\n",
    "        nn.init.normal_(self.values, std=0.02)  # apparently from lample et al 2019, CAN't FIND ITS REFERENCE\n",
    "\n",
    "        # Weight Matrix One\n",
    "        self.W1 = nn.Linear(d_model, self.value_dim, bias=False)\n",
    "\n",
    "        # Weight Matrix Two\n",
    "        self.W2 = nn.Linear(self.value_dim, d_model, bias=False)\n",
    "\n",
    "        # Silu Activation Function\n",
    "        self.silu = nn.SiLU()\n",
    "\n",
    "        # QK-Normalisation, \n",
    "        # NOTE:I think its more a general backbone design choice for memory layer, potentially place this after residual connection as we are using interleaved architecture (at end of this gated memory layer)\n",
    "        \"\"\"\n",
    "        NOTE: This is a technique used to stabilize training and improve convergence in transformer models. \n",
    "        \"\"\"\n",
    "        self.qk_norm = nn.RMSNorm(self.subkey_dim) \n",
    "        \n",
    "        # Top-K Selection\n",
    "        \"\"\"\n",
    "        NOTE: Can experiment with this value, as it may affect performance and memory usage. \n",
    "        \"\"\"\n",
    "        self.top_k = top_k\n",
    "\n",
    "        # Softmax\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "\n",
    "    def lookup_memory(self, query):\n",
    "\n",
    "        # 1. Apply normalisation for cosine similarity style lookup\n",
    "        k1 = self.qk_norm(self.subkey_one)\n",
    "        k2 = self.qk_norm(self.subkey_two)\n",
    "\n",
    "        # 2. Get similarity subkey scores with query\n",
    "        sim_scores_1 = query @ k1.T\n",
    "        sim_scores_2  = query @ k2.T\n",
    "        all_scores = sim_scores_1.unsqueeze(-1) + sim_scores_2.unsqueeze(-2)\n",
    "\n",
    "        # 3. Cartesian Product Search:\n",
    "        all_scores = all_scores.view(*all_scores.shape[:-2], -1) \n",
    "        \n",
    "        # 4. Select the final top-k combinations\n",
    "        top_k_scores, top_k_indices = torch.topk(all_scores, self.top_k, dim=-1)\n",
    "        \n",
    "        # 5. Retrieve Values and Aggregate \n",
    "        s = self.softmax(top_k_scores) \n",
    "\n",
    "        # 6. Gather Values and Aggregate: NOTE: Using EmbeddingBag! \n",
    "        # TODO: Make CUDA kernel to quicken EmbeddingBag solution\n",
    "        flat_indices = top_k_indices.view(-1, self.top_k)\n",
    "        flat_weights = s.view(-1, self.top_k)\n",
    "        y_flat = F.embedding_bag(flat_indices, self.values, per_sample_weights=flat_weights, mode='sum')\n",
    "        \n",
    "        return y_flat.view(*query.shape[:-1], self.value_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        q = self.query(x)\n",
    "        q = self.qk_norm(q)\n",
    "\n",
    "        y = self.lookup_memory(q)\n",
    "                \n",
    "        m_plus = self.silu(self.W1(x))\n",
    "        m_plus = y * m_plus\n",
    "        m_plus = self.W2(m_plus)\n",
    "\n",
    "        return m_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1d046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_model_performance(model, device, name=\"Model\"):\n",
    "    \"\"\"Profiles a single forward and backward pass to see memory/FLOP tradeoffs.\"\"\"\n",
    "    print(f\"\\n--- Profiling {name} ---\")\n",
    "    model.eval()\n",
    "    inputs = torch.randn(1, 3, 224, 224).to(device)\n",
    "    \n",
    "\n",
    "     # [ProfilerActivity.CPU, ProfilerActivity.CUDA] if torch.cuda.is_available() else [ProfilerActivity.CPU],\n",
    "    with profile(\n",
    "        activities = [ProfilerActivity.CPU],  \n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True\n",
    "    ) as prof:\n",
    "        with record_function(\"forward_pass\"):\n",
    "            output = model(inputs)\n",
    "        with record_function(\"backward_pass\"):\n",
    "            loss = output.sum()\n",
    "            loss.backward()\n",
    "            \n",
    "    # Sorted by CUDA time if available, else CPU time\n",
    "    sort_by = \"gpu_time_total\" if torch.cuda.is_available() else \"cpu_time_total\"\n",
    "    print(prof.key_averages().table(sort_by=sort_by, row_limit=10))\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for images, labels in loader:\n",
    "        print(\"hi\")\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    return running_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    return running_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def run_comparison(dense_model, memory_model, train_loader, test_loader, device, epochs=5):\n",
    "    # Detect device (Note: MPS for Mac is an option, but profiler support varies)\n",
    "\n",
    "    # Fixed keys to match your storage logic\n",
    "    results = {'dense': {'val_loss': [], 'val_acc': []}, 'memory': {'val_loss': [], 'val_acc': []}}\n",
    "    speed_comp = {dense_model : 0.0, memory_model : 0.0}\n",
    "    for name, model in [('dense', dense_model), ('memory', memory_model)]:\n",
    "        print(f\"\\nStarting training for {name}...\")\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        start_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "            print(f\"Epoch {epoch+1}: train loss: {train_loss}, train acc: {train_acc}\")\n",
    "            val_loss, val_acc = validate(model, test_loader, criterion, device)\n",
    "            \n",
    "            results[name]['val_loss'].append(val_loss)\n",
    "            results[name]['val_acc'].append(val_acc)\n",
    "            print(f\"Epoch {epoch+1}: Val Acc {val_acc:.2f}%\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        speed_comp[model] = end_time - start_time\n",
    "\n",
    "        \n",
    "\n",
    "    return results, speed_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5336c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Accuracy\n",
    "def plot_results(results):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(results['dense']['acc'], label='Dense Baseline (Pre-trained ViT)')\n",
    "    plt.plot(results['memory']['acc'], label='Memory+ Adapter ViT')\n",
    "    plt.title('CIFAR-100 Validation Accuracy')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5e41fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available(): \n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "        \n",
    "device    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45a8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.Grayscale(num_output_channels=3), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_set = datasets.FashionMNIST(root='./data_dir', train=True, download=True, transform=transform)\n",
    "test_set = datasets.FashionMNIST(root='./data_dir', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9699f2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Models\n",
    "dense_model = timm.create_model('mobilenetv2_100', pretrained=True, num_classes=10, cache_dir=\"./models_dir\").to(device)\n",
    "memory_model = timm.create_model('mobilenetv2_100', pretrained=True, num_classes=10, cache_dir = \"./models_dir\").to(device)\n",
    "\n",
    "d_model = dense_model.embed_dim\n",
    "memory_slots = 256**2 \n",
    "memory_model.blocks[6].mlp = MemoryPlusLayer(d_model=d_model, memory_slots=memory_slots).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b4d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"VERY IMPORTANT NOTE: \n",
    "\n",
    "Base model is 17x faster than Memory+ (1024**2 memory slots) ViT!!!! \n",
    "NEED CUSTOM KERNEL FOR EMBEDDINGBAG SOLUTION TO SPEED THIS UP, \n",
    "AS THIS IS THE BOTTLENECK IN THE MEMORY LAYER.\n",
    "\n",
    "Hoever found memory slot size 256**2 to be near performance of baseline!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# PROFILE MODELS BEFORE TRAINING TO SEE MEMORY/FLOP TRADEOFFS\n",
    "profile_model_performance(dense_model, device, name=\"Dense Baseline\")\n",
    "profile_model_performance(memory_model, device, name=\"Memory+ Adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623f648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res, speeds = run_comparison(dense_model, memory_model, train_loader, test_loader, device, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543e065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1636bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in speeds.key():\n",
    "    print(f\"{m} Training Time: {speeds[m]:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
